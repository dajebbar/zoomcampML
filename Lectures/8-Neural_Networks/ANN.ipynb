{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "ylUM0iQJtsph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3FjVMIrj_TX"
      },
      "outputs": [],
      "source": [
        "def initialisation(n0, n1, n2):\n",
        "  w1 = np.random.randn(n1, n0)\n",
        "  b1 = np.zeros(n1, 1)\n",
        "\n",
        "  w2 = np.random.randn(n2, n1)\n",
        "  b2 = np.zeros(n2, 1)\n",
        "\n",
        "  params = {\n",
        "      'w1': w1,\n",
        "      'b1': b1,\n",
        "      'w2': w2;\n",
        "      'b2': b2,\n",
        "  }\n",
        "  return params"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagation(X, params):\n",
        "  z1 = params['z1']\n",
        "  b1 = params['b1']\n",
        "  z2 = params['z2']\n",
        "  b2 = params['b2']\n",
        "  \n",
        "  z1 = X @ w1 + b1\n",
        "  a1 = 1 / (1 + np.exp(-z1))\n",
        "\n",
        "  z2 = X @ w2 + b2\n",
        "  a1 = 1 / (1 + np.exp(-z2))\n",
        "\n",
        "  activations = {\n",
        "      'a1': a1,\n",
        "      'a2': a2\n",
        "  }\n",
        "\n",
        "  return activations"
      ],
      "metadata": {
        "id": "bmwJ8B4fvVDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def back_propagation(X, y, params, activations):\n",
        "  m = y.shape[1]\n",
        "  a1 = activations['a1']\n",
        "  a2 = activations['a2']\n",
        "  w2 = params['w2']\n",
        "\n",
        "  dz2 = a2 - y\n",
        "  dw2 = 1/m * dz2 @ a1.T\n",
        "  db2 = 1 / m * np.sum(dz2, axis=1, keepdims=True)\n",
        "\n",
        "  dz1 = w2.T @ dz2 * a1 * (1 - a1)\n",
        "  dw1 = 1 / m * dz1 @ X.T\n",
        "  db1 = 1 / m np.sum(dz1, axis=1, keepdims=True)\n",
        "\n",
        "  gradients = {\n",
        "      'dw1': dw1,\n",
        "      'db1': db1,\n",
        "      'dw2': dw2,\n",
        "      'db2': db2,\n",
        "  }\n",
        "  return gradients"
      ],
      "metadata": {
        "id": "tjuZokFWvUuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update(gradients, params, lr):\n",
        "  w1 = params['w1']\n",
        "  b1 = params['b1']\n",
        "  w2 = params['w2']\n",
        "  b2 = params['b2']\n",
        "\n",
        "  dw1 = gradients['dw1']\n",
        "  db1 = gradients['db1']\n",
        "  dw2 = gradients['dw2']\n",
        "  db2 = gradients['db2']\n",
        "\n",
        "  w1 = w1 - lr * dw1\n",
        "  b1 = b1 - lr * db1\n",
        "  w2 = w2 - lr * dw2\n",
        "  b2 = b2 - lr * db2\n",
        "\n",
        "  params = {\n",
        "      'w1': w1,\n",
        "      'b1': b1,\n",
        "      'w2': w2;\n",
        "      'b2': b2,\n",
        "  }\n",
        "  \n",
        "  return params"
      ],
      "metadata": {
        "id": "ErjNQW7U2ViZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, params):\n",
        "  activations = forward_propagation(X, params)\n",
        "  a2 = activations['a2']\n",
        "  return a2 >= .5"
      ],
      "metadata": {
        "id": "_t8BEXfp3oH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def neural_network(X, y, n1=32, lr=0.01, n_iter=1000):\n",
        "  # params initialization\n",
        "  n0 = X.shape[0]\n",
        "  n2 = y.shape[0]\n",
        "  np.random.seed(42)\n",
        "  params = initialisation(n0, n1, n2)\n",
        "\n",
        "  train_loss = []\n",
        "  train_acc = []\n",
        "  history = []\n",
        "\n",
        "  # gradient descent\n",
        "  for i in tqdm(range(n_iter)):\n",
        "    activations = forward_propagation(X, params)\n",
        "    a2 = activations['a2']\n",
        "\n",
        "    # plot learning curve\n",
        "    train_loss.append(metrics.log_loss(y.flatten(), a2.flatten()))\n",
        "    y_pred = predict(X, params)\n",
        "    train_acc.append(metrics.accuracy_score(y.flatten(), y_pred.flatten()))\n",
        "\n",
        "    history.append([params.copy(), train_loss, train_acc, i])\n",
        "\n",
        "    # update\n",
        "    gradients = back_propagation(X, y, params, activations)\n",
        "    params = update(gradients, params, lr)\n",
        "  \n",
        "  plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_loss, label='train loss')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_acc, label='train acc')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "  \n",
        "  return params\n"
      ],
      "metadata": {
        "id": "YTcTypgm3n5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def neural_network_custom(X_train, y_train, X_test, y_test, lr=.01, n_iter=1000):\n",
        "  # Initialisation of weights w and biais b\n",
        "  # params initialization\n",
        "  n0 = X_train.shape[0]\n",
        "  n2 = y_train.shape[0]\n",
        "  np.random.seed(42)\n",
        "  params = initialisation(n0, n1, n2)\n",
        "  train_loss = []\n",
        "  test_loss = []\n",
        "  train_acc = []\n",
        "  test_acc = []\n",
        "\n",
        "  for i in tqdm(range(n_iter)):\n",
        "    # activation == model\n",
        "    activations = forward_propagation(X_train, params)\n",
        "    a2 = activations['a2']\n",
        "\n",
        "    if i % 10 == 0:\n",
        "      # Train\n",
        "      train_loss.append(metrics.log_loss(y_train.flatten(), a2.flatten()))\n",
        "      y_pred = predict(X_train, params)\n",
        "      train_acc.append(metrics.accuracy_score(y_train.flatten(), y_pred.flatten()))\n",
        "      \n",
        "      # Test\n",
        "      activations_test = forward_propagation(X_test, params)\n",
        "      a2_test = activations_test['a2']\n",
        "      test_loss.append(metrics.log_loss(y_test.flatten(), a2.flatten()))\n",
        "      y_pred_test = predict(X_test, params)\n",
        "      test_acc.append(metrics.accuracy_score(y_test.flatten(), y_pred_test.flatten()))\n",
        "      \n",
        "    # update\n",
        "    gradients = back_propagation(X_train, y_train, params, activations)\n",
        "    params = update(gradients, params, lr)\n",
        "  \n",
        "  plt.figure(figsize=(12, 4))\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(train_loss, label='train loss')\n",
        "  plt.plot(test_loss, label='test loss')\n",
        "  plt.legend()\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(train_acc, label='train acc')\n",
        "  plt.plot(test_acc, label='test acc')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  return params\n"
      ],
      "metadata": {
        "id": "3nFPelAw_ds1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}